{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#discriminator architecture\n\nclass discriminator(nn.Module):\n    def __init__(self,input_channels=3):\n        super(discriminator,self).__init__()\n\n\n        \n        self.block_1=nn.Sequential(\n                nn.Conv2d(input_channels*2,64,kernel_size=(4,4),stride=(2,2),padding=1),\n                nn.LeakyReLU(0.2)\n                )\n        self.block_2=nn.Sequential(\n                nn.Conv2d(64,128,kernel_size=(4,4),stride=(2,2),padding=1),\n                nn.BatchNorm2d(128),\n                nn.LeakyReLU(0.2)\n                )\n        self.block_3=nn.Sequential(\n                nn.Conv2d(128,256,kernel_size=(4,4),stride=(2,2),padding=1),\n                nn.BatchNorm2d(256),\n                nn.LeakyReLU(0.2)\n                )\n        self.block_4=nn.Sequential(\n                nn.Conv2d(256,512,kernel_size=(4,4),stride=(2,2),padding=1),\n                nn.BatchNorm2d(512),\n                nn.LeakyReLU(0.2)\n                )\n        self.block_5=nn.ZeroPad2d((1, 0, 1, 0))\n        self.block_6=nn.Sequential(\n                nn.Conv2d(512,1,kernel_size=(4,4),stride=(1,1),padding=1),\n                nn.Sigmoid()\n                )\n    def forward(self,x,y):\n        input=torch.cat([x,y],dim=1)\n        out=self.block_1(input)\n        out=self.block_2(out)\n        out=self.block_3(out)\n        out=self.block_4(out)\n        out=self.block_5(out)\n        out=self.block_6(out)\n      \n        \n        return out\n            \n    \ndef test():\n    x=torch.randn((1,3,286,286))\n    y=torch.randn((1,3,286,286))\n    preds=discriminator(input_channels=3)\n    output=preds(x,y)\n    print(output.shape)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generator architecture\n\nclass encoder(nn.Module):\n    def __init__(self,input_channels,output_channels,normalize=True,dropout=0.0):\n        super(encoder,self).__init__()\n        \n        layers=[nn.Conv2d(input_channels,output_channels,4,2,1,bias=False)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(output_channels))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model=nn.Sequential(*layers)\n        \n    def forward(self,x):\n        return self.model(x)\n    \nclass decoder(nn.Module):\n    def __init__(self,input_channels,output_channels,dropout=0.0):\n        super(decoder,self).__init__()\n        layers=[\n            nn.ConvTranspose2d(input_channels,output_channels,4,2,1,bias=False),\n            nn.InstanceNorm2d(output_channels),\n            nn.ReLU(inplace=True)\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n            \n        self.model=nn.Sequential(*layers)\n    \n    def forward(self,x,skip_input):\n        x=self.model(x)\n        x=torch.cat((x,skip_input),1) # torch.cat(tensors, dim=0, *, out=None) â†’ Tensor(here 1 represents concatenation along columns)\n        \n        return x\n    \n    \nclass generator(nn.Module):\n    def __init__(self,input_channels=3,output_channels=3):\n        super(generator,self).__init__()\n        # encoder model: C64-C128-C256-C512-C512-C512-C512-C512\n        self.down1=self.model(input_channels,64,normalize=False)\n        self.down2=self.model(64,128)\n        self.down3=self.model(128,256)\n        self.down4=self.model(256,512)\n        self.down5=self.model(512,512)\n        self.down6=self.model(512,512)\n        self.down7=self.model(512,512)\n        \n        self.bottle_neck=self.model(512,512,normalize=False) #bottleneck layer\n        \n        # decoder model: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n        self.up1=self.model(512,512,dropout=0.5)\n        self.up2=self.model(512*2,512,dropout=0.5)\n        self.up3=self.model(512*2,512,dropout=0.5)\n        self.up4=self.model(512*2,512,dropout=0.5)\n        self.up5=self.model(512*2,256)\n        self.up6=self.model(256*2,128)\n        self.up7=self.model(128*2,64)\n        \n        self.final_up=nn.Sequential(\n        nn.ConvTranspose2d(64*2,out_channels,4,2,1),\n        nn.Tanh(),\n        )\n        \n        def forward(self,x):\n            d1=self.down1(x)\n            d2=self.down2(d1)\n            d3=self.down3(d2) \n            d4=self.down4(d3)\n            d5=self.down5(d4)\n            d6=self.down6(d5)\n            d7=self.down7(d6)\n            \n            bottle_neck=self.bottle_neck(d7)\n            \n            u1=self.up1(bottle_neck,d7)\n            u2=self.up2(u1,d6)\n            u3=self.up3(u2,d5)\n            u4=self.up4(u3,d4)\n            u5=self.up5(u4,d3)\n            u6=self.up6(u5,d2)\n            u7=self.up7(u6,d1)\n            \n            return self.final_up(u7)\n                ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Arial2Map(Dataset):\n    def __init__(self, root, transform=None):\n        self.root = root\n        self.file = root + '/maps/maps/train/'\n        self.transform = transform\n    \n    def __len__(self):\n        return len(os.listdir(self.file))\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.file + str(idx+1) + '.jpg')\n        w, h = img.size\n        img_A = img.crop((0, 0, w/2, h))\n        img_B = img.crop((w/2, 0, w, h))\n        \n        if self.transform:\n            img_A = self.transform(img_A)\n            img_B = self.transform(img_B)\n        return {'arial':img_A, 'map':img_B}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n                    transforms.Resize((256,256)),\n                    transforms.ToTensor()\n            ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = Arial2Map('../input/pix2pix-dataset', transform=transform)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(train_data[71]['arial'].permute(1,2,0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = torch.utils.data.DataLoader(dataset=train_data,\n                                        shuffle=True,\n                                        batch_size=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nepochs = 200\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"D_loss=[]\nG_loss=[]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G=generator().to(device)\nD=discriminator().to(device)\ncriterion_gan=torch.nn.MSELoss()\ncriterion_pixelwise=torch.nn.L1Loss()\nlambda_pixel=100\noptimizer_G=torch.optim.Adam(G.parameters(),lr=0.0002,betas=(0.5,0.999))\noptimizer_D=torch.optim.Adam(D.parameters(),lr=0.0002,betas=(0.5,0.999))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    for i,imgs in enumerate(dataloader):\n        real_arials=imgs['arial'].to(device)\n        real_maps=imgs['map'].to(device)\n        \n        valid=torch.ones(real_arials.shape[0],1,16,16).to(device)\n        fake=torch.zeros(real_arials.shape[0],1,16,16).to(device)\n        \n        \n        # train generator\n        output_gen=G(real_arials)\n        pred_fake=D(output_gen,real_arials)\n        gan_loss=criterion_gan(pred_fake,valid)\n        \n        #pixelwise loss\n        L1_loss=criterion_pixelwise(output_gen,real_maps)\n        \n        Gen_loss=gan_loss+lambda_pixel*L1_loss\n        \n        G.zero_grad()\n        Gen_loss.backward()\n        \n        optimizer_G.step()\n        \n        \n        #train discriminator\n        \n        pred_real=D(real_maps,real_arials)\n        loss_real=criterion_gan(pred_real,valid)\n        \n        pred_fake=D(output_gen.detach(),real_arials)\n        loss_fake=criterion_gan(pred_fake,fake)\n        \n        Disc_loss=(loss_real+loss_fake)/2\n        \n        D.zero_grad()\n        Disc_loss.backward()\n        \n        optimizer_D.step()\n        \n        D_loss.append(Disc_loss.item())\n        G_loss.append(Gen_loss.item())\n        \n        if(i+1)%500==0:\n            sys.stdout.write(\n                \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f adv: %f]\\n\"\n                % (\n                    epoch,\n                    epochs,\n                    i,\n                    len(dataloader),\n                    Disc_loss.item(),\n                    Gen_loss.item(),\n                    gan_loss.item(),\n                )\n            )\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = G(train_data[89]['arial'].unsqueeze(0).to(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(out.detach().cpu().squeeze(0).permute(1,2,0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(train_data[89]['arial'].permute(1,2,0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(train_data[89]['map'].permute(1,2,0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}